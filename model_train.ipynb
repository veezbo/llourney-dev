{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "# dataset = load_dataset('lambdalabs/pokemon-blip-captions')\n",
    "dataset = load_dataset(\"atasoglu/flickr8k-dataset\", data_dir=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_id', 'image_path', 'captions'],\n",
       "    num_rows: 6000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': '2730819220_b58af1119a',\n",
       " 'image_path': '/home/veezbo/.cache/huggingface/datasets/downloads/extracted/8c0281a0d6433d492cf11514ee37574297a203fdd2f114c2b7edb98bf297a371/Flicker8k_Dataset/2730819220_b58af1119a.jpg',\n",
       " 'captions': ['a little girl ends up at the bottom of the slide .',\n",
       "  'a little girl is just reaching the bottom of a playground slide',\n",
       "  'A little girl lands at the bottom of a slide .',\n",
       "  'A young girl reaches the bottom of a slide .',\n",
       "  'a young girl sliding down a tan plastic slide']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get('train')[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(dataset.get('train')[110]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from typing import Literal\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "CONTEXT_SIZE = 32\n",
    "# PAD_TOKEN = '[PAD]'\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "MODEL = 'gpt2-large'\n",
    "DATASET_TO_USE: Literal['pokemon', 'flickr'] = 'flickr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO(1): Is this causing problems? Should we actually use the pad token\n",
    "# TODO(1): Should we do left padding instead to keep the important data together\n",
    "# TODO(1): Actually validate that the attention mask coming from the tokenizer is correctly masking just the pad tokens\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL)\n",
    "# tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "\n",
    "# def resize_images(batch):\n",
    "#     # Specify your target size\n",
    "#     target_size = (224, 224)\n",
    "#     # Load the image\n",
    "#     image = Image.open(batch['image_path'])\n",
    "#     # Resize the image\n",
    "#     image = image.resize(target_size)\n",
    "#     # Convert the image to a numpy array and normalize pixel values to [0, 1]\n",
    "#     batch['image'] = np.array(image) / 255.0\n",
    "#     return batch\n",
    "\n",
    "# def tokenize_and_pad_texts(batch):\n",
    "#     # Tokenize the texts\n",
    "#     tokenized_batch = tokenizer(batch['text'], padding='longest', truncation=True, max_length=CONTEXT_SIZE)\n",
    "#     return tokenized_batch\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMAGE_SIZE, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),  # NOTE: This shuffles from (H, W, C) to (C, H, W)\n",
    "        transforms.Normalize([0.5], [0.5]),  # TODO(2): Adjust normalization if needed. Can do linear transformation using transforms.lambda\n",
    "    ]\n",
    ")\n",
    "\n",
    "def tokenize_and_resize_pokemon(examples):\n",
    "    tokenizer_output = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=CONTEXT_SIZE)\n",
    "    return {\n",
    "        'input_ids': tokenizer_output['input_ids'],\n",
    "        'attention_mask': tokenizer_output['attention_mask'],\n",
    "        'image': train_transforms(examples['image']),\n",
    "    }\n",
    "\n",
    "def tokenize_and_resize_flickr(examples):\n",
    "    tokenizer_output = tokenizer(examples['captions'][0], padding='max_length', truncation=True, max_length=CONTEXT_SIZE)\n",
    "    # print(examples['captions'][0])\n",
    "    return {\n",
    "        'input_ids': tokenizer_output['input_ids'],\n",
    "        'attention_mask': tokenizer_output['attention_mask'],\n",
    "        'image': train_transforms(Image.open(examples['image_path'])),\n",
    "        'text': examples['captions'][0]\n",
    "    }\n",
    "\n",
    "\n",
    "match DATASET_TO_USE:\n",
    "    case 'pokemon':\n",
    "        f = f\"./data/pokemon_train_dataset_{MODEL}_{IMAGE_SIZE}_{CONTEXT_SIZE}.hf\"\n",
    "        if os.path.exists(f):\n",
    "            train_dataset = load_from_disk(f)\n",
    "        else:\n",
    "            train_dataset = dataset['train'].map(tokenize_and_resize_pokemon).with_format('torch')\n",
    "            train_dataset.save_to_disk(f)\n",
    "    case 'flickr':\n",
    "        f = f\"./data/flickr_train_dataset_{MODEL}_{IMAGE_SIZE}_{CONTEXT_SIZE}.hf\"\n",
    "        if os.path.exists(f):\n",
    "            train_dataset = load_from_disk(f)\n",
    "        else:\n",
    "            train_dataset = dataset['train'].map(tokenize_and_resize_flickr).with_format('torch')\n",
    "            train_dataset.save_to_disk(f)\n",
    "            \n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max(len(x.split(' ')) for x in train_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': ['3391209042_d2de8a8978',\n",
       "  '3684518763_f3490b647a',\n",
       "  '2272426567_9e9fb79db0',\n",
       "  '362316425_bda238b4de'],\n",
       " 'image_path': ['/home/veezbo/.cache/huggingface/datasets/downloads/extracted/8c0281a0d6433d492cf11514ee37574297a203fdd2f114c2b7edb98bf297a371/Flicker8k_Dataset/3391209042_d2de8a8978.jpg',\n",
       "  '/home/veezbo/.cache/huggingface/datasets/downloads/extracted/8c0281a0d6433d492cf11514ee37574297a203fdd2f114c2b7edb98bf297a371/Flicker8k_Dataset/3684518763_f3490b647a.jpg',\n",
       "  '/home/veezbo/.cache/huggingface/datasets/downloads/extracted/8c0281a0d6433d492cf11514ee37574297a203fdd2f114c2b7edb98bf297a371/Flicker8k_Dataset/2272426567_9e9fb79db0.jpg',\n",
       "  '/home/veezbo/.cache/huggingface/datasets/downloads/extracted/8c0281a0d6433d492cf11514ee37574297a203fdd2f114c2b7edb98bf297a371/Flicker8k_Dataset/362316425_bda238b4de.jpg'],\n",
       " 'captions': [('Two figures stand in a snowy setting wearing white and hot pink outfits , gazing towards a mountain .',\n",
       "   'A guy in gold chains , a black top , and gold shorts is walking with his arms outstretched in a parade .',\n",
       "   'A dog walks across the snow .',\n",
       "   'A man and a woman wearing masks embrace at an outdoor festival .'),\n",
       "  ('Two people dressed in white ropes and pink gloves look at the mountain .',\n",
       "   'A young man raises his arms as he marches in a parade .',\n",
       "   'A dog walks through the snow in the daylight .',\n",
       "   'A man and woman dressed up dance together outdoors .'),\n",
       "  ('Two people dressed up in white sheets and pink gloves stare at a snow covered glacier off in the distance .',\n",
       "   'A young man with gold short shorts and a black top is walking in a parade .',\n",
       "   'Black and brown dog approaching camera across snow .',\n",
       "   'two people acting out a scene in front of an audience .'),\n",
       "  ('Two people in white robes look at a snowy mountain .',\n",
       "   'Man in gold jewelry and shorts by a parade .',\n",
       "   'Black dog runs through snow with head down',\n",
       "   'Two people in masks dancing .'),\n",
       "  ('Two people wearing white and pink plastic gesture at a mountain in the distance .',\n",
       "   'The man in the gold lame shorts is wearing many gold necklaces .',\n",
       "   'The large , dark colored dog is walking through the snow .',\n",
       "   'Two people in middle eastern garb embrace on a crowded lawn .')],\n",
       " 'input_ids': tensor([[ 7571,  5538,  1302,   287,   257, 46742,  4634,  5762,  2330,   290,\n",
       "           3024, 11398, 27655,   837, 50234,  3371,   257,  8598,   764, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256],\n",
       "         [   32,  3516,   287,  3869, 14659,   837,   257,  2042,  1353,   837,\n",
       "            290,  3869, 22078,   318,  6155,   351,   465,  5101,   503, 49729,\n",
       "            287,   257, 16134,   764, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256],\n",
       "         [   32,  3290, 11114,  1973,   262,  6729,   764, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256],\n",
       "         [   32,   582,   290,   257,  2415,  5762, 20680, 12553,   379,   281,\n",
       "          15162, 10876,   764, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'image': tensor([[[[-0.0039, -0.0039, -0.0039,  ..., -0.3255, -0.3176, -0.3176],\n",
       "           [-0.0039, -0.0118, -0.0039,  ..., -0.3098, -0.3098, -0.3098],\n",
       "           [-0.0039, -0.0118, -0.0118,  ..., -0.2941, -0.2863, -0.2941],\n",
       "           ...,\n",
       "           [ 0.2392,  0.2392,  0.2627,  ...,  0.9294,  0.9216,  0.9373],\n",
       "           [ 0.2706,  0.2784,  0.2706,  ...,  0.9294,  0.9216,  0.9294],\n",
       "           [ 0.2784,  0.3020,  0.2549,  ...,  0.8745,  0.8667,  0.8745]],\n",
       " \n",
       "          [[ 0.1294,  0.1294,  0.1294,  ..., -0.1529, -0.1529, -0.1529],\n",
       "           [ 0.1294,  0.1216,  0.1294,  ..., -0.1529, -0.1608, -0.1529],\n",
       "           [ 0.1294,  0.1216,  0.1216,  ..., -0.1451, -0.1451, -0.1529],\n",
       "           ...,\n",
       "           [ 0.3176,  0.3098,  0.3725,  ...,  0.9608,  0.9529,  0.9686],\n",
       "           [ 0.3490,  0.3490,  0.3804,  ...,  0.9451,  0.9373,  0.9451],\n",
       "           [ 0.3647,  0.3647,  0.3647,  ...,  0.8824,  0.8745,  0.8824]],\n",
       " \n",
       "          [[ 0.2863,  0.2784,  0.2627,  ..., -0.0118, -0.0039, -0.0039],\n",
       "           [ 0.2863,  0.2784,  0.2706,  ..., -0.0039, -0.0039, -0.0118],\n",
       "           [ 0.2863,  0.2784,  0.2784,  ..., -0.0039, -0.0039, -0.0118],\n",
       "           ...,\n",
       "           [ 0.4588,  0.4588,  0.4902,  ...,  0.9843,  0.9765,  0.9843],\n",
       "           [ 0.4902,  0.4980,  0.4980,  ...,  0.9765,  0.9686,  0.9686],\n",
       "           [ 0.5059,  0.5137,  0.4824,  ...,  0.9216,  0.9059,  0.9137]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0824, -0.1765, -0.3804,  ...,  0.1608,  0.1608,  0.1451],\n",
       "           [ 0.0510, -0.2235, -0.3176,  ...,  0.1059,  0.1059,  0.1137],\n",
       "           [ 0.0196, -0.2784, -0.3098,  ...,  0.0980,  0.0980,  0.1765],\n",
       "           ...,\n",
       "           [-0.2706, -0.1608,  0.2000,  ...,  0.0667,  0.0667,  0.0745],\n",
       "           [-0.2078, -0.2863, -0.2627,  ...,  0.0824,  0.0902,  0.0824],\n",
       "           [-0.2157, -0.2549, -0.2314,  ...,  0.0667,  0.0667,  0.0588]],\n",
       " \n",
       "          [[-0.2235, -0.4275, -0.6078,  ..., -0.2078, -0.2235, -0.2000],\n",
       "           [-0.2314, -0.4667, -0.5686,  ..., -0.2078, -0.2078, -0.2000],\n",
       "           [-0.2471, -0.5059, -0.5373,  ..., -0.1451, -0.1451, -0.1373],\n",
       "           ...,\n",
       "           [-0.2549, -0.2314, -0.0588,  ..., -0.0745, -0.0667, -0.0745],\n",
       "           [-0.2078, -0.2784, -0.2784,  ..., -0.0902, -0.0980, -0.0902],\n",
       "           [-0.2549, -0.2549, -0.2157,  ..., -0.0824, -0.0980, -0.0745]],\n",
       " \n",
       "          [[-0.2549, -0.4588, -0.6314,  ..., -0.2471, -0.2627, -0.2549],\n",
       "           [-0.2941, -0.5216, -0.5922,  ..., -0.2314, -0.2314, -0.2471],\n",
       "           [-0.3020, -0.5529, -0.5608,  ..., -0.1843, -0.1922, -0.1765],\n",
       "           ...,\n",
       "           [-0.2392, -0.2549, -0.2314,  ..., -0.1216, -0.1137, -0.1216],\n",
       "           [-0.2157, -0.2549, -0.2706,  ..., -0.1137, -0.1216, -0.1216],\n",
       "           [-0.2471, -0.2157, -0.2157,  ..., -0.1059, -0.1216, -0.1137]]],\n",
       " \n",
       " \n",
       "         [[[-0.7098, -0.6784, -0.7333,  ..., -0.7490, -0.7804, -0.6941],\n",
       "           [-0.6784, -0.6863, -0.6784,  ..., -0.8118, -0.7725, -0.7098],\n",
       "           [-0.7333, -0.7255, -0.6627,  ..., -0.8039, -0.7176, -0.6471],\n",
       "           ...,\n",
       "           [ 1.0000,  1.0000,  1.0000,  ...,  0.9922,  0.9922,  0.9922],\n",
       "           [ 1.0000,  1.0000,  0.9922,  ...,  0.9922,  0.9922,  0.9765],\n",
       "           [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9843,  0.9922]],\n",
       " \n",
       "          [[-0.7333, -0.6941, -0.7804,  ..., -0.7725, -0.8118, -0.7333],\n",
       "           [-0.7569, -0.7098, -0.7255,  ..., -0.8196, -0.7725, -0.7098],\n",
       "           [-0.7882, -0.7255, -0.6941,  ..., -0.8118, -0.7490, -0.6941],\n",
       "           ...,\n",
       "           [ 1.0000,  1.0000,  1.0000,  ...,  0.9922,  0.9922,  0.9843],\n",
       "           [ 1.0000,  1.0000,  0.9922,  ...,  0.9843,  0.9922,  0.9922],\n",
       "           [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9843,  0.9922]],\n",
       " \n",
       "          [[-0.7333, -0.6706, -0.7569,  ..., -0.7882, -0.8039, -0.6784],\n",
       "           [-0.6941, -0.6627, -0.7020,  ..., -0.8431, -0.8118, -0.6863],\n",
       "           [-0.7490, -0.7333, -0.6941,  ..., -0.8196, -0.7569, -0.6078],\n",
       "           ...,\n",
       "           [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  1.0000,  1.0000],\n",
       "           [ 1.0000,  1.0000,  1.0000,  ...,  0.9765,  0.9922,  0.9922],\n",
       "           [ 0.9922,  0.9922,  1.0000,  ...,  0.9843,  0.9922,  0.9922]]],\n",
       " \n",
       " \n",
       "         [[[-0.0431, -0.0824, -0.1216,  ..., -0.1843, -0.2471, -0.3882],\n",
       "           [-0.1529, -0.2000, -0.2314,  ..., -0.0431, -0.0980, -0.2471],\n",
       "           [-0.2471, -0.3176, -0.3176,  ...,  0.1686,  0.0980, -0.0588],\n",
       "           ...,\n",
       "           [-0.6314, -0.8431, -0.9765,  ..., -0.5843, -0.5922, -0.6000],\n",
       "           [-0.6549, -0.7961, -0.9451,  ..., -0.5922, -0.5843, -0.6078],\n",
       "           [-0.6784, -0.8118, -0.9529,  ..., -0.6000, -0.5922, -0.6078]],\n",
       " \n",
       "          [[ 0.0667,  0.0275,  0.0353,  ..., -0.1059, -0.2000, -0.3882],\n",
       "           [-0.0275, -0.1137, -0.0980,  ...,  0.0118, -0.0745, -0.2706],\n",
       "           [-0.0980, -0.2078, -0.2078,  ...,  0.1922,  0.0980, -0.0902],\n",
       "           ...,\n",
       "           [-0.9451, -0.9608, -0.9922,  ..., -0.6863, -0.7020, -0.7020],\n",
       "           [-0.9608, -0.9686, -0.9843,  ..., -0.7020, -0.6863, -0.7020],\n",
       "           [-0.9765, -0.9686, -0.9843,  ..., -0.6784, -0.6941, -0.7098]],\n",
       " \n",
       "          [[-0.4667, -0.5373, -0.5922,  ..., -0.7333, -0.7255, -0.7020],\n",
       "           [-0.5059, -0.5451, -0.5765,  ..., -0.6863, -0.6549, -0.6549],\n",
       "           [-0.5451, -0.6078, -0.6000,  ..., -0.7020, -0.6863, -0.6549],\n",
       "           ...,\n",
       "           [-0.9843, -0.9922, -0.9843,  ..., -0.8196, -0.8275, -0.8353],\n",
       "           [-0.9922, -0.9686, -0.9843,  ..., -0.8196, -0.8118, -0.8353],\n",
       "           [-0.9843, -0.9843, -1.0000,  ..., -0.8353, -0.8353, -0.8431]]]]),\n",
       " 'text': ['Two figures stand in a snowy setting wearing white and hot pink outfits , gazing towards a mountain .',\n",
       "  'A guy in gold chains , a black top , and gold shorts is walking with his arms outstretched in a parade .',\n",
       "  'A dog walks across the snow .',\n",
       "  'A man and a woman wearing masks embrace at an outdoor festival .']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image_id', 'image_path', 'captions', 'input_ids', 'attention_mask', 'image', 'text'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader)).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))['image'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from typing import Literal\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "NUM_EPOCHS = 4096\n",
    "LEARNING_RATE = 3e-5\n",
    "NUM_DIFFUSION_TIMESTEPS = 100\n",
    "LATENT_DIM = IMAGE_SIZE // 8\n",
    "EVAL_ITERATIONS = 100\n",
    "PREDICTION_TYPE: Literal['epsilon', 'sample'] = 'epsilon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "from diffusers.optimization import get_constant_schedule\n",
    "from model import LLourney\n",
    "\n",
    "model = LLourney()\n",
    "model.to(DEVICE)\n",
    "scheduler = DDPMScheduler(num_train_timesteps=NUM_DIFFUSION_TIMESTEPS, prediction_type=PREDICTION_TYPE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "lr_schedule = get_constant_schedule(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3548738560\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = next(iter(train_loader))\n",
    "# elimage = thing['image'].to(DEVICE)\n",
    "# eltext_id = thing[\"input_ids\"].to(DEVICE)\n",
    "# elpad_mask = thing[\"attention_mask\"].to(DEVICE)\n",
    "# print(elimage.shape)\n",
    "# print(eltext_id.shape)\n",
    "# print(elpad_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thing = next(iter(train_loader))\n",
    "# elimage = torch.stack([thing['image'][0]] * BATCH_SIZE, dim=0).to(DEVICE)\n",
    "# eltext_id = torch.stack([thing['input_ids'][0]] * BATCH_SIZE, dim=0).to(DEVICE)\n",
    "# elpad_mask = torch.stack([thing['attention_mask'][0]] * BATCH_SIZE, dim=0).to(DEVICE)\n",
    "\n",
    "# elimage = torch.ones([BATCH_SIZE, 3, 128, 128]).to(DEVICE)\n",
    "# eltext_id = torch.stack([thing['input_ids'][0]] * BATCH_SIZE, dim=0).to(DEVICE)\n",
    "# elpad_mask = torch.stack([thing['attention_mask'][0]] * BATCH_SIZE, dim=0).to(DEVICE)\n",
    "\n",
    "# print(elimage.shape)\n",
    "# print(eltext_id.shape)\n",
    "# print(elpad_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint if available\n",
    "step_i = 0\n",
    "CHECKPOINT_FILE_BASE_PATH = f\"model_checkpoint_flickr_llm_100difftimestep_epsilon_32context_patchdim2_imgdim256_gptlarge\"\n",
    "\n",
    "checkpoints = glob.glob(f\"{CHECKPOINT_FILE_BASE_PATH}*\")\n",
    "if len(checkpoints):\n",
    "    steps = [int(re.findall(f\"{CHECKPOINT_FILE_BASE_PATH}_step_(\\d+).pt\", checkpoint)[0]) for checkpoint in checkpoints]\n",
    "    max_step = max(steps)\n",
    "    CHECKPOINT_FILE_PATH = f\"{CHECKPOINT_FILE_BASE_PATH}_step_{max_step}.pt\"\n",
    "    print(f\"LOADING CHECKPOINT FROM: {CHECKPOINT_FILE_PATH}\")\n",
    "    checkpoint = torch.load(CHECKPOINT_FILE_PATH, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    step_i = checkpoint['step_i']\n",
    "\n",
    "# model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3548738560\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_100: 0.9997423416376114\n",
      "step_200: 0.7362519389390946\n",
      "step_300: 0.6154901888966561\n",
      "step_400: 0.6148268097639084\n",
      "step_500: 0.5997130024433136\n",
      "step_600: 0.6181740722060204\n",
      "step_700: 0.5955809274315834\n",
      "step_800: 0.5821639862656594\n",
      "step_900: 0.5743371146917343\n",
      "step_1000: 0.5724548202753067\n",
      "step_1100: 0.5857533740997315\n",
      "step_1200: 0.5876998236775398\n",
      "step_1300: 0.5632940590381622\n",
      "step_1400: 0.5459697914123535\n",
      "step_1500: 0.5556989958882332\n",
      "step_1600: 0.5453901273012162\n",
      "step_1700: 0.5867823848128318\n",
      "step_1800: 0.5449166482686997\n",
      "step_1900: 0.555638926923275\n",
      "step_2000: 0.5670486050844192\n",
      "step_2100: 0.5322461694478988\n",
      "step_2200: 0.5595515170693397\n",
      "step_2300: 0.5347107174992561\n",
      "step_2400: 0.545967561006546\n",
      "step_2500: 0.5386095994710922\n",
      "step_2600: 0.5517191290855408\n",
      "step_2700: 0.5441773712635041\n",
      "step_2800: 0.569552618265152\n",
      "step_2900: 0.5321801468729973\n",
      "step_3000: 0.570259618461132\n",
      "step_3100: 0.547101169526577\n",
      "step_3200: 0.5556990075111389\n",
      "step_3300: 0.5461532717943192\n",
      "step_3400: 0.542904820740223\n",
      "step_3500: 0.5552638480067253\n",
      "step_3600: 0.556496265232563\n",
      "step_3700: 0.5147373446822167\n",
      "step_3800: 0.5438202178478241\n",
      "step_3900: 0.5671104580163956\n",
      "step_4000: 0.535360224545002\n",
      "step_4100: 0.5337606580555438\n",
      "step_4200: 0.5482785430550575\n",
      "step_4300: 0.5275590792298317\n",
      "step_4400: 0.5449096491932869\n",
      "step_4500: 0.5611389049887657\n",
      "step_4600: 0.5517645168304444\n",
      "step_4700: 0.5536579695343972\n",
      "step_4800: 0.529479465931654\n",
      "step_4900: 0.5394685611128807\n",
      "step_5000: 0.5325442957878113\n",
      "step_5100: 0.5492483007907868\n",
      "step_5200: 0.5349560996890068\n",
      "step_5300: 0.5464576518535614\n",
      "step_5400: 0.5229256093502045\n",
      "step_5500: 0.5288621756434441\n",
      "step_5600: 0.5285922142863274\n",
      "step_5700: 0.5359677225351334\n",
      "step_5800: 0.5305838297307491\n",
      "step_5900: 0.5648373031616211\n",
      "step_6000: 0.5324818301200867\n",
      "step_6100: 0.549266020655632\n",
      "step_6200: 0.5204702144861222\n",
      "step_6300: 0.5360632970929146\n",
      "step_6400: 0.5394956988096237\n",
      "step_6500: 0.5345480445027352\n",
      "step_6600: 0.5247506248950958\n",
      "step_6700: 0.5550391671061515\n",
      "step_6800: 0.5483137530088424\n",
      "step_6900: 0.5546751199662685\n",
      "step_7000: 0.5178245520591735\n",
      "step_7100: 0.5408519475162029\n",
      "step_7200: 0.534525974392891\n",
      "step_7300: 0.5276556414365768\n",
      "step_7400: 0.5332693514227868\n",
      "step_7500: 0.519948852956295\n",
      "step_7600: 0.522379714846611\n",
      "step_7700: 0.5206665435433387\n",
      "step_7800: 0.5315144869685173\n",
      "step_7900: 0.5328982079029083\n",
      "step_8000: 0.5202833935618401\n",
      "step_8100: 0.5248588842153549\n",
      "step_8200: 0.5463383677601814\n",
      "step_8300: 0.5265207920968532\n",
      "step_8400: 0.538103748857975\n",
      "step_8500: 0.5347670385241509\n",
      "step_8600: 0.5133471766114235\n",
      "step_8700: 0.5285531222820282\n",
      "step_8800: 0.5367932370305062\n",
      "step_8900: 0.5209426498413086\n",
      "step_9000: 0.5023382453620434\n",
      "step_9100: 0.541357279419899\n",
      "step_9200: 0.5149021747708321\n",
      "step_9300: 0.5284862729907036\n",
      "step_9400: 0.5315957048535347\n",
      "step_9500: 0.5366230365633965\n",
      "step_9600: 0.5330366656184197\n",
      "step_9700: 0.5308861221373081\n",
      "step_9800: 0.5232048380374908\n",
      "step_9900: 0.5239687892794609\n",
      "step_10000: 0.5048293310403824\n",
      "step_10100: 0.5457311967015266\n",
      "step_10200: 0.5535085210204125\n",
      "step_10300: 0.5190560910105705\n",
      "step_10400: 0.5252296662330628\n",
      "step_10500: 0.5331580582261085\n",
      "step_10600: 0.5432268358767033\n",
      "step_10700: 0.5284614688158036\n",
      "step_10800: 0.5232978346943855\n",
      "step_10900: 0.5110229697823524\n",
      "step_11000: 0.5372389574348927\n",
      "step_11100: 0.5196891787648201\n",
      "step_11200: 0.5257186529040336\n",
      "step_11300: 0.5551838526129722\n",
      "step_11400: 0.5682563364505768\n",
      "step_11500: 0.535619689822197\n",
      "step_11600: 0.5085118651390076\n",
      "step_11700: 0.5295239675045014\n",
      "step_11800: 0.5212828940153122\n",
      "step_11900: 0.519166721701622\n",
      "step_12000: 0.5437295207381249\n",
      "step_12100: 0.540167530477047\n",
      "step_12200: 0.5250289517641068\n",
      "step_12300: 0.5291190686821937\n",
      "step_12400: 0.5265944416821003\n",
      "step_12500: 0.5273917400836945\n",
      "step_12600: 0.5201113682985306\n",
      "step_12700: 0.5359496691823006\n",
      "step_12800: 0.5200979787111283\n",
      "step_12900: 0.5211254140734672\n",
      "step_13000: 0.5133869431912899\n",
      "step_13100: 0.5236636000871658\n",
      "step_13200: 0.5102233731746674\n",
      "step_13300: 0.5286090952157975\n",
      "step_13400: 0.5147176697850228\n",
      "step_13500: 0.5348501333594322\n",
      "step_13600: 0.5139553990960121\n",
      "step_13700: 0.5280900019407272\n",
      "step_13800: 0.5523760887980461\n",
      "step_13900: 0.5262178769707679\n",
      "step_14000: 0.5259830233454704\n",
      "step_14100: 0.517131961286068\n",
      "step_14200: 0.5303827668726444\n",
      "step_14300: 0.5118886412680149\n",
      "step_14400: 0.5343387860059738\n",
      "step_14500: 0.523926647901535\n",
      "step_14600: 0.5113099706172943\n",
      "step_14700: 0.5274694821238518\n",
      "step_14800: 0.5188202510774136\n",
      "step_14900: 0.511100624203682\n",
      "step_15000: 0.5341230216622352\n",
      "step_15100: 0.5100117006897926\n",
      "step_15200: 0.5276285785436631\n",
      "step_15300: 0.509760208427906\n",
      "step_15400: 0.5174907258152962\n",
      "step_15500: 0.5129582008719444\n",
      "step_15600: 0.5104080806672573\n",
      "step_15700: 0.5294037362933159\n",
      "step_15800: 0.5408993619680404\n",
      "step_15900: 0.5006557115912438\n",
      "step_16000: 0.5379290407896042\n",
      "step_16100: 0.5135710960626603\n",
      "step_16200: 0.48990674152970315\n",
      "step_16300: 0.5290439873933792\n",
      "step_16400: 0.5106823398172855\n",
      "step_16500: 0.53578727632761\n",
      "step_16600: 0.5211524075269699\n",
      "step_16700: 0.5321885174512864\n",
      "step_16800: 0.514155980348587\n",
      "step_16900: 0.5374984860420227\n",
      "step_17000: 0.5130410268902779\n",
      "step_17100: 0.5202491523325443\n",
      "step_17200: 0.539190085530281\n",
      "step_17300: 0.519569956511259\n",
      "step_17400: 0.5179045552015304\n",
      "step_17500: 0.5136280846595764\n",
      "step_17600: 0.5122071143984794\n",
      "step_17700: 0.5031809337437153\n",
      "step_17800: 0.5083501052856445\n",
      "step_17900: 0.5227173218131065\n",
      "step_18000: 0.5118196499347687\n",
      "step_18100: 0.5268820878863335\n",
      "step_18200: 0.5210206472873687\n",
      "step_18300: 0.520734643638134\n",
      "step_18400: 0.5045215421915055\n",
      "step_18500: 0.4981563849747181\n",
      "step_18600: 0.5368725317716598\n",
      "step_18700: 0.4936994379758835\n",
      "step_18800: 0.523385988920927\n",
      "step_18900: 0.5221576482057572\n",
      "step_19000: 0.49819122195243837\n",
      "step_19100: 0.5209390288591385\n",
      "step_19200: 0.5270303928852081\n",
      "step_19300: 0.5101779547333717\n",
      "step_19400: 0.5188423091173172\n",
      "step_19500: 0.49930238008499145\n",
      "step_19600: 0.5237800458073616\n",
      "step_19700: 0.5374643135070801\n",
      "step_19800: 0.523699100613594\n",
      "step_19900: 0.5129303114116192\n",
      "step_20000: 0.5238956074416637\n",
      "step_20100: 0.5342916050553321\n",
      "step_20200: 0.5146571923792362\n",
      "step_20300: 0.5118325513601303\n",
      "step_20400: 0.5154112321138382\n",
      "step_20500: 0.5220548811554909\n",
      "step_20600: 0.5091421899199485\n",
      "step_20700: 0.5258816975355148\n",
      "step_20800: 0.49160860538482665\n",
      "step_20900: 0.5157820653915405\n",
      "step_21000: 0.5010789740085602\n",
      "step_21100: 0.5224428489804268\n",
      "step_21200: 0.52635666847229\n",
      "step_21300: 0.5186657693982124\n",
      "step_21400: 0.49754558950662614\n",
      "step_21500: 0.5246392279863358\n",
      "step_21600: 0.49564256101846693\n",
      "step_21700: 0.5069883033633232\n",
      "step_21800: 0.5250420868396759\n",
      "step_21900: 0.5234519731998444\n",
      "step_22000: 0.5251076847314835\n",
      "step_22100: 0.5156541311740875\n",
      "step_22200: 0.5247378620505333\n",
      "step_22300: 0.5148524984717369\n",
      "step_22400: 0.5351675802469253\n",
      "step_22500: 0.5029923179745674\n",
      "step_22600: 0.49335974276065825\n",
      "step_22700: 0.4992714461684227\n",
      "step_22800: 0.5104528081417083\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m PREDICTION_TYPE:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# Use the model to predict the noise \u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m         latent_noise \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_latent_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pad_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         loss_node \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(latent_noise\u001b[38;5;241m.\u001b[39mfloat(), Z, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/llourney/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/llourney/model.py:140\u001b[0m, in \u001b[0;36mLLourney.forward\u001b[0;34m(self, latent_image, input_ids, timestep, text_pad_mask)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# print(\"transformer_emb pre llama:\", transformer_emb.shape)\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Push unified embedding through Llama transformer blocks\u001b[39;00m\n\u001b[1;32m    139\u001b[0m total_pad_mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(text_pad_mask, (\u001b[38;5;241m0\u001b[39m, NUM_PATCHES\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, T+1+NP)\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m transformer_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_pad_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state  \u001b[38;5;66;03m# (B, S, C)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# print(\"transformer_emb post llama:\", transformer_emb.shape)\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# TODO(3): Use einops to maybe do the reshaping and shuffling in a single step while being a lot more obvious\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# NOTE: Recover the latent image channels from hidden dimension\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Pluck out last NP tokens correspoding to patch embeddings\u001b[39;00m\n\u001b[1;32m    148\u001b[0m projected_img_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_img_proj(transformer_emb[:, \u001b[38;5;241m-\u001b[39mNUM_PATCHES:, :])  \u001b[38;5;66;03m# (S[B, :-NP, C]) @ (C, LC*L*L/NP) -> (B, NP, LC*L*L/NP)\u001b[39;00m\n",
      "File \u001b[0;32m~/llourney/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/llourney/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    890\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    892\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 900\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/llourney/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/llourney/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:391\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    389\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    390\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 391\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/llourney/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/llourney/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:332\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 332\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    335\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/llourney/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:202\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    199\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(causal_mask, attn_weights\u001b[38;5;241m.\u001b[39mto(attn_weights\u001b[38;5;241m.\u001b[39mdtype), mask_value)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "\n",
    "model.train()\n",
    "loss_evals = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        step_i += 1\n",
    "        \n",
    "        # Set image, text, and pad mask to randomly sampled batch\n",
    "        image = batch[\"image\"].to(DEVICE)\n",
    "        text_id = batch[\"input_ids\"].to(DEVICE)\n",
    "        pad_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        # Set image, text, and pad mask to single batch below\n",
    "        # image = elimage\n",
    "        # text_id = eltext_id\n",
    "        # pad_mask = elpad_mask\n",
    "        \n",
    "        batch_size = image.shape[0]\n",
    "\n",
    "        # Use VAE to encode image in latent space\n",
    "        latent_image = model.encode_image(image)\n",
    "        assert latent_image.shape[-1] == LATENT_DIM\n",
    "\n",
    "        # Sample gaussian with same shape as latent image\n",
    "        Z = torch.randn_like(latent_image).to(DEVICE)\n",
    "\n",
    "        # Sample timesteps, one for each item in the batch\n",
    "        T = torch.randint(1, NUM_DIFFUSION_TIMESTEPS, (batch_size,)).to(DEVICE)\n",
    "        T = T.long()\n",
    "\n",
    "        # Transform input images to noisy images now\n",
    "        noisy_latent_image = scheduler.add_noise(latent_image, Z, T)\n",
    "\n",
    "        match PREDICTION_TYPE:\n",
    "            case 'epsilon':\n",
    "                # Use the model to predict the noise \n",
    "                latent_noise = model(noisy_latent_image, text_id, T, text_pad_mask=pad_mask)\n",
    "                # Calculate the loss\n",
    "                loss_node = F.mse_loss(latent_noise.float(), Z, reduction='mean')\n",
    "            case 'sample':\n",
    "                # Use the model to predict the latent image\n",
    "                denoised_latent_image = model(noisy_latent_image, text_id, T, text_pad_mask=pad_mask)\n",
    "                # Calculate the loss\n",
    "                loss_node = F.mse_loss(denoised_latent_image.float(), latent_image, reduction='mean')        \n",
    "\n",
    "        # Backpropagate\n",
    "        loss_node.backward()\n",
    "        optimizer.step()\n",
    "        lr_schedule.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_evals.append(loss_node.detach().item())\n",
    "        if step_i % EVAL_ITERATIONS == 0:\n",
    "            print(f'step_{step_i}: {sum(loss_evals)/len(loss_evals)}')\n",
    "            # print('memory allocated:', torch.cuda.memory_allocated(), 'max memory allocated:', torch.cuda.memory_allocated())\n",
    "            # print('memory reserved:', torch.cuda.memory_reserved(), 'max memory reserved:', torch.cuda.max_memory_reserved())\n",
    "            loss_evals = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_FILE_PATH = f\"{CHECKPOINT_FILE_BASE_PATH}_step_{step_i}.pt\"\n",
    "print(CHECKPOINT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22899"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to model_checkpoint_flickr_llm_100difftimestep_epsilon_32context_patchdim2_imgdim256_gptlarge_step_22899.pt\n"
     ]
    }
   ],
   "source": [
    "# Save checkpoint\n",
    "CHECKPOINT_FILE_PATH = f\"{CHECKPOINT_FILE_BASE_PATH}_step_{step_i}.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'step_i': step_i,\n",
    "}, CHECKPOINT_FILE_PATH)\n",
    "print(f\"Saved checkpoint to {CHECKPOINT_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INFERENCE_STEPS = 10\n",
    "DEL_OPTIMIZER: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thing['captions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEL_OPTIMIZER:\n",
    "    del optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('memory allocated:', torch.cuda.memory_allocated(), 'max memory allocated:', torch.cuda.memory_allocated())\n",
    "print('memory reserved:', torch.cuda.memory_reserved(), 'max memory reserved:', torch.cuda.max_memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.pipelines.pipeline_utils import numpy_to_pil\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "from model import LLourney\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# model = LLourney()\n",
    "\n",
    "# inprogress = []\n",
    "\n",
    "# if DEL_OPTIMIZER:\n",
    "#     del optimizer\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# NOTE: THERE'S SOMETHING REALLY BAD HAPPENING AT GENERATION TIME WITH MEMORY\n",
    "\n",
    "def encode_prompt(prompt):\n",
    "    tokenizer_output = tokenizer(prompt, padding='max_length', truncation=True, max_length=CONTEXT_SIZE)\n",
    "    input_ids = torch.LongTensor(tokenizer_output[\"input_ids\"]).to(DEVICE)\n",
    "    attention_mask = torch.Tensor(tokenizer_output[\"attention_mask\"]).to(DEVICE)\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(prompt: str) -> Image:\n",
    "    # model.eval()\n",
    "    prompt = [prompt]\n",
    "\n",
    "    print('memory allocated:', torch.cuda.memory_allocated(), 'max memory allocated:', torch.cuda.memory_allocated())\n",
    "    print('memory reserved:', torch.cuda.memory_reserved(), 'max memory reserved:', torch.cuda.max_memory_reserved())\n",
    "\n",
    "    # NOTE: Replace Z with below in order to conditon generation on single image from the batch 'thing'\n",
    "    # latent_conditional_image = model.encode_image(elimage[:1, :, :, :])\n",
    "    \n",
    "    Z = torch.randn(len(prompt), 4, LATENT_DIM, LATENT_DIM, device=DEVICE)\n",
    "    scheduler.set_timesteps(NUM_INFERENCE_STEPS)\n",
    "    timesteps = scheduler.timesteps\n",
    "\n",
    "    input_ids, attention_mask = encode_prompt(prompt)\n",
    "    latent_images = Z\n",
    "    # latent_images = latent_conditional_image\n",
    "    print(f\"Latent noise shape: {latent_images.shape}\")\n",
    "    \n",
    "    for i, t in enumerate(timesteps):\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if i % (len(timesteps) // 10) == 0:\n",
    "            print(f\"Diffused to timestep {i}/{len(timesteps)}\")\n",
    "            # print('memory allocated:', torch.cuda.memory_allocated(), 'max memory allocated:', torch.cuda.memory_allocated())\n",
    "            # print('memory reserved:', torch.cuda.memory_reserved(), 'max memory reserved:', torch.cuda.max_memory_reserved())\n",
    "            \n",
    "        latent_images = scheduler.scale_model_input(latent_images, t)\n",
    "        batched_t = torch.cat([torch.tensor([t])] * len(prompt), dim=0).to(DEVICE)\n",
    "        \n",
    "        model_output = model(latent_images, input_ids, batched_t, text_pad_mask=attention_mask)\n",
    "        latent_images = scheduler.step(model_output, t, latent_images, return_dict=False)[0]\n",
    "\n",
    "        # decoded_imagess = model.decode_image_latents(latent_images)\n",
    "        # image = [numpy_to_pil(img) for img in decoded_imagess]\n",
    "        # inprogress.append(image[0][0])\n",
    "\n",
    "    print(latent_images.shape)\n",
    "    \n",
    "    decoded_images = model.decode_image_latents(latent_images)\n",
    "    image = [numpy_to_pil(img) for img in decoded_images]\n",
    "\n",
    "    return image[0][0]\n",
    "\n",
    "# Generate some images\n",
    "# for _ in range(10):\n",
    "#     image = generate(\"A girl in a cowboy hat with a sheep on a leash\")\n",
    "#     display(image)\n",
    "# image = generate(\"a drawing of a red and yellow pokemon character\")\n",
    "# display(image)\n",
    "# image = generate(\"a very cute looking pokemon with a big beak\")\n",
    "# display(image)\n",
    "for i in range(len(thing['text'])):\n",
    "    image = generate(thing['text'][i])\n",
    "    print(thing['text'][i])\n",
    "    display(image)\n",
    "    display(Image.open(thing['image_path'][i]))\n",
    "\n",
    "# print(latent_images.shape)\n",
    "# decoded_images = model.decode_image_latents(latent_images)\n",
    "# image = [numpy_to_pil(img) for img in decoded_images]\n",
    "\n",
    "# # Generate some images\n",
    "# prompt = [\"pokeymans\", \"big dog with cricket bat\", \"florgs is GAYYYYYY\"]\n",
    "# Z = torch.randn(len(prompt), 4, LATENT_DIM, LATENT_DIM, device=DEVICE)\n",
    "# scheduler = DDPMScheduler(num_train_timesteps=NUM_TRAIN_TIMESTEPS)\n",
    "# scheduler.set_timesteps(NUM_INFERENCE_STEPS)\n",
    "# timesteps = scheduler.timesteps\n",
    "\n",
    "# def encode_prompt(prompt):\n",
    "#     tokenizer_output = tokenizer(prompt, padding='max_length', truncation=True, max_length=CONTEXT_SIZE)\n",
    "#     input_ids = torch.LongTensor(tokenizer_output[\"input_ids\"]).to(DEVICE)\n",
    "#     attention_mask = torch.Tensor(tokenizer_output[\"attention_mask\"]).to(DEVICE)\n",
    "#     return input_ids, attention_mask\n",
    "\n",
    "# input_ids, attention_mask = encode_prompt(prompt)\n",
    "# latent_images = Z\n",
    "# print(f\"Latent noise shape: {latent_images.shape}\")\n",
    "# for i, t in enumerate(timesteps):\n",
    "#     batched_t = torch.cat([torch.tensor([t])] * len(prompt), dim=0)\n",
    "#     print(f\"Batched t shape: {batched_t.shape}\")\n",
    "#     print(f\"Input ids 666: {input_ids.shape}\")\n",
    "#     print(f\"Attention mask 666: {attention_mask.shape}\")\n",
    "#     model_output = model(latent_images, input_ids, batched_t, text_pad_mask=attention_mask)\n",
    "\n",
    "#     latent_images = scheduler.step(model_output, t, latent_images).prev_sample\n",
    "\n",
    "# print(latent_images.shape)\n",
    "# decoded_images = model.decode_image_latents(latent_images)\n",
    "# image = [numpy_to_pil(img) for img in decoded_images]\n",
    "\n",
    "# display(image[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_images = model.encode_image(thing[\"image\"].to(DEVICE))\n",
    "decoded_images = model.decode_image_latents(latent_images)\n",
    "image = [numpy_to_pil(img) for img in decoded_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in inprogress:\n",
    "    display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "image.save(f\"pred_img_{step_i}_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "\n",
    "scheduler = DDPMScheduler()\n",
    "\n",
    "scheduler.set_timesteps(100)\n",
    "\n",
    "timesteps = scheduler.timesteps\n",
    "\n",
    "print(timesteps)\n",
    "print(timesteps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.llama.config.n_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.llama.config.n_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.pipelines.pipeline_utils import numpy_to_pil\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "# Test what adding latent noise makes VAE look like\n",
    "\n",
    "# # First, display image without any latent noise\n",
    "# display(Image.open(thing['image_path'][0]))\n",
    "# latent_images = model.encode_image(thing[\"image\"].to(DEVICE))\n",
    "# decoded_images = model.decode_image_latents(latent_images)\n",
    "# image = [numpy_to_pil(img) for img in decoded_images]\n",
    "# display(image[0][0])\n",
    "\n",
    "# # Then, display image after having added latent noise\n",
    "# Z = torch.randn_like(latent_images)\n",
    "# latent_images_noise_rand = latent_images + Z\n",
    "# decoded_images_noise_rand = model.decode_image_latents(latent_images_noise_rand)\n",
    "# image_noisy_rand = [numpy_to_pil(img) for img in decoded_images_noise_rand]\n",
    "# display(image_noisy_rand[0][0])\n",
    "\n",
    "# # Then, display image after having added latent noise with scheduler as in the model\n",
    "# t = torch.randint(0, NUM_DIFFUSION_TIMESTEPS, (1,), device=DEVICE, dtype=torch.long)\n",
    "# # t = torch.tensor([300], device=DEVICE, dtype=torch.long)\n",
    "# print(\"timestep t:\", t)\n",
    "# latent_images_noise_sched = scheduler.add_noise(latent_images, Z, t)\n",
    "# decoded_images_noise_sched = model.decode_image_latents(latent_images_noise_sched)\n",
    "# image_noisy_sched = [numpy_to_pil(img) for img in decoded_images_noise_sched]\n",
    "# display(image_noisy_sched[0][0])\n",
    "\n",
    "# norm_z = torch.norm(Z)\n",
    "# norm_latent_images = torch.norm(latent_images)\n",
    "# norm_latent_images_noise_rand = torch.norm(latent_images_noise_rand)\n",
    "# print(f\"Norm of z: {norm_z}\")\n",
    "# print(f\"Norm of latent_images: {norm_latent_images}\")\n",
    "# print(f\"Norm of latent_images_noise_rand: {norm_latent_images_noise_rand}\")\n",
    "# mse_dist = F.mse_loss(norm_z, norm_latent_images)\n",
    "# mse_dist2 = F.mse_loss(norm_z, norm_latent_images_noise_rand)\n",
    "# print(f\"MSE between z and latent_images: {mse_dist}\")\n",
    "# print(f\"MSE between z and latent_images_noise_rand: {mse_dist2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
